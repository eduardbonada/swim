{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Sessions\n",
    "\n",
    "This script processes the sessions data directly downloaded from Garmin Connect \n",
    "and compiles all lengths into a single csv file with the following columns:\n",
    "    \n",
    "> datetime  |  interval  |  length  |  style  |  distance  |  time  |  strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_single_session(session_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Process a single swimming session.\n",
    "    Reads the file provided as argument and returns the processed session as a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Start processing file {}'.format(session_path))\n",
    "    \n",
    "    # load single data file\n",
    "    session_datetime = datetime.strptime(session_path.split(sep='_')[2][:12], '%Y%m%d%H%M')\n",
    "    session_raw = pd.read_csv(session_path)\n",
    "    #print('{} lines in raw file'.format(len(session_raw)))\n",
    "    \n",
    "    # drop rows: summary of interval, summary of session, error lengths with '--' in style column\n",
    "    session_processed = session_raw.drop(session_raw[session_raw['Lengths'] > 1.0].index)\n",
    "\n",
    "    # drop error lengths with '--' in style column\n",
    "    session_processed = session_processed.drop(session_processed[session_processed['Swim Stroke'] == '--'].index)\n",
    "    \n",
    "    # get columns we need and set final columns\n",
    "    session_final = session_processed[['Split','Distance', 'Time', 'Swim Stroke', 'Total Strokes']]\n",
    "    session_final['datetime'] = session_datetime\n",
    "    session_final['interval'] = session_final.apply( \n",
    "        lambda x: x['Split'].split('.')[0] if '.' in x['Split'] else '',\n",
    "        axis=1\n",
    "    )\n",
    "    session_final['length'] = session_final.apply( \n",
    "        lambda x: x['Split'].split('.')[1] if '.' in x['Split'] else '',\n",
    "        axis=1\n",
    "    )\n",
    "    session_final['style'] = session_final.apply(\n",
    "        lambda x: x['Swim Stroke'] if x['Swim Stroke'] == x['Swim Stroke'] else x['Split'] ,\n",
    "        axis=1\n",
    "    )\n",
    "    session_final['distance'] = session_final.apply( \n",
    "        lambda x: x['Distance'],\n",
    "        axis=1\n",
    "    )\n",
    "    session_final['time'] = session_final.apply(\n",
    "        lambda x: (datetime.strptime(x['Time'], '%H:%M:%S.%f')-datetime.strptime('00:00:00.000', '%H:%M:%S.%f')).total_seconds(),\n",
    "        axis=1\n",
    "    )\n",
    "    session_final['strokes'] = session_final.apply(\n",
    "        lambda x: x['Total Strokes'] if x['Total Strokes'] != '--' else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # TODO: add interval and length in rest rows\n",
    "\n",
    "    # drop original columns\n",
    "    session_final = session_final.drop(['Split','Distance', 'Time', 'Swim Stroke', 'Total Strokes'], axis=1)\n",
    "    \n",
    "    #print('{} lines in final file'.format(len(session_final)))\n",
    "    \n",
    "    # return final dataframe\n",
    "    return session_final\n",
    "    \n",
    "    # dump dataframe into csv\n",
    "    #session_final.to_csv('lengths.csv', header=False, index=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all activity files in data\n",
    "sessions_folder = 'sessions'\n",
    "sessions_files = sorted([f for f in listdir('sessions') if ( isfile(join('sessions', f)) and f.startswith('activity'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing file sessions/activity_1255537501_201607141156.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduard/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/eduard/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/eduard/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/eduard/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/eduard/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/eduard/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/eduard/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing file sessions/activity_1259535835_201607151216.csv\n",
      "Start processing file sessions/activity_1379959220_201609061806.csv\n",
      "Start processing file sessions/activity_1379959252_201609121854.csv\n",
      "Start processing file sessions/activity_1379959270_201609151815.csv\n",
      "Start processing file sessions/activity_1379959299_201609191837.csv\n",
      "Start processing file sessions/activity_1379959323_201609221800.csv\n",
      "Start processing file sessions/activity_1379959351_201609261744.csv\n",
      "Start processing file sessions/activity_1400932066_201609291737.csv\n",
      "Start processing file sessions/activity_1400932077_201610031737.csv\n",
      "Start processing file sessions/activity_1400932086_201610051827.csv\n",
      "Start processing file sessions/activity_1400932096_201610090939.csv\n",
      "Start processing file sessions/activity_1408867640_201610121001.csv\n",
      "Start processing file sessions/activity_1408867644_201610150825.csv\n",
      "Start processing file sessions/activity_1422683148_201610171834.csv\n",
      "Start processing file sessions/activity_1422683153_201610191908.csv\n",
      "Start processing file sessions/activity_1422683155_201610211823.csv\n",
      "Start processing file sessions/activity_1422683161_201610241858.csv\n",
      "Start processing file sessions/activity_1433572553_201610271817.csv\n",
      "Start processing file sessions/activity_1447150466_201611061001.csv\n",
      "Start processing file sessions/activity_1447150470_201611081925.csv\n",
      "Start processing file sessions/activity_1447150473_201611121913.csv\n",
      "Start processing file sessions/activity_1578236250_201611161922.csv\n",
      "Start processing file sessions/activity_1578236259_201611200920.csv\n",
      "Start processing file sessions/activity_1578236269_201611221941.csv\n",
      "Start processing file sessions/activity_1578236281_201611251938.csv\n",
      "Start processing file sessions/activity_1578236291_201612021958.csv\n",
      "Start processing file sessions/activity_1578236301_201612051911.csv\n",
      "Start processing file sessions/activity_1578236309_201612101245.csv\n",
      "Start processing file sessions/activity_1578236316_201612121934.csv\n",
      "Start processing file sessions/activity_1578236327_201612161929.csv\n",
      "Start processing file sessions/activity_1578236336_201702041840.csv\n",
      "Start processing file sessions/activity_1578236353_201702171935.csv\n",
      "Start processing file sessions/activity_1591212141_201702210708.csv\n",
      "Start processing file sessions/activity_1591212155_201702230708.csv\n",
      "Start processing file sessions/activity_1641884846_201702251910.csv\n",
      "Start processing file sessions/activity_1641884863_201702280708.csv\n",
      "Start processing file sessions/activity_1641884879_201703020708.csv\n",
      "Start processing file sessions/activity_1641884885_201703041316.csv\n",
      "Start processing file sessions/activity_1641884904_201703060708.csv\n",
      "Start processing file sessions/activity_1641884923_201703080706.csv\n",
      "Start processing file sessions/activity_1641884944_201703102003.csv\n",
      "Start processing file sessions/activity_1641884961_201703150708.csv\n",
      "Start processing file sessions/activity_1641884973_201703170706.csv\n",
      "Start processing file sessions/activity_1641884988_201703200706.csv\n",
      "Start processing file sessions/activity_1641885002_201703220707.csv\n",
      "Start processing file sessions/activity_1641885017_201703240706.csv\n",
      "Start processing file sessions/activity_1660154228_201703282023.csv\n",
      "Start processing file sessions/activity_1660154240_201703312024.csv\n",
      "Start processing file sessions/activity_1724244653_201704062016.csv\n",
      "Start processing file sessions/activity_1724244679_201704081933.csv\n",
      "Start processing file sessions/activity_1724244693_201704101925.csv\n",
      "Start processing file sessions/activity_1724244708_201704131921.csv\n",
      "Start processing file sessions/activity_1724244731_201704192013.csv\n",
      "Start processing file sessions/activity_1724244752_201704261956.csv\n",
      "Start processing file sessions/activity_1724244827_201704291355.csv\n",
      "Start processing file sessions/activity_1724244846_201705021955.csv\n",
      "Start processing file sessions/activity_1724244873_201705052015.csv\n",
      "Start processing file sessions/activity_1724244918_201705082022.csv\n",
      "Start processing file sessions/activity_1863118723_201705131349.csv\n",
      "Start processing file sessions/activity_1863118732_201705162022.csv\n",
      "Start processing file sessions/activity_1863118746_201707091253.csv\n",
      "Start processing file sessions/activity_1863118751_201707132017.csv\n",
      "Start processing file sessions/activity_1863118756_201707161258.csv\n",
      "Start processing file sessions/activity_1863118759_201707182014.csv\n",
      "Start processing file sessions/activity_1885181568_201707202035.csv\n",
      "Start processing file sessions/activity_1885181590_201707282024.csv\n",
      "Start processing file sessions/activity_1885181624_201707301258.csv\n",
      "Start processing file sessions/activity_1890368475_201708012014.csv\n",
      "Start processing file sessions/activity_1899397019_201708032042.csv\n",
      "Start processing file sessions/activity_1899397029_201708061242.csv\n",
      "Start processing file sessions/activity_2058187229_201709110937.csv\n",
      "Start processing file sessions/activity_2058187240_201709130709.csv\n",
      "Start processing file sessions/activity_2058187252_201709150721.csv\n",
      "Start processing file sessions/activity_2058187269_201709170911.csv\n",
      "Start processing file sessions/activity_2058187282_201709200718.csv\n",
      "Start processing file sessions/activity_2058187292_201709220723.csv\n",
      "Start processing file sessions/activity_2058187307_201709270722.csv\n",
      "Start processing file sessions/activity_2058187326_201709280728.csv\n",
      "Start processing file sessions/activity_2058187341_201710020724.csv\n",
      "Start processing file sessions/activity_2058187358_201710050720.csv\n",
      "Start processing file sessions/activity_2058187374_201710070941.csv\n",
      "Start processing file sessions/activity_2158436732_201710081715.csv\n",
      "Start processing file sessions/activity_2158436750_201710110748.csv\n",
      "Start processing file sessions/activity_2158436767_201710121711.csv\n",
      "Start processing file sessions/activity_2158436785_201710151706.csv\n",
      "Start processing file sessions/activity_2158436793_201710180720.csv\n",
      "Start processing file sessions/activity_2158436806_201710211538.csv\n",
      "Start processing file sessions/activity_2321883239_201710230721.csv\n",
      "Start processing file sessions/activity_2321883254_201710250719.csv\n",
      "Start processing file sessions/activity_2321883259_201710270726.csv\n",
      "Start processing file sessions/activity_2321883275_201710291504.csv\n",
      "Start processing file sessions/activity_2321883295_201711010912.csv\n",
      "Start processing file sessions/activity_2321883304_201711080724.csv\n",
      "Start processing file sessions/activity_2340520314_201711110911.csv\n",
      "Start processing file sessions/activity_2340520325_201711130729.csv\n",
      "Start processing file sessions/activity_2340520331_201711170725.csv\n"
     ]
    }
   ],
   "source": [
    "# loop files and process all sessions\n",
    "all_sessions = pd.DataFrame()\n",
    "for sf in sessions_files:#[5:6]:\n",
    "    session_df = process_single_session(join(sessions_folder, sf))\n",
    "    all_sessions = all_sessions.append(session_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset the indexes after concatenation \n",
    "all_sessions = all_sessions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5638 rows\n"
     ]
    }
   ],
   "source": [
    "print('{} rows'.format(len(all_sessions)))\n",
    "#print('{} rows | {} distance'.format(len(all_sessions), sum(all_sessions['distance'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Manually) Mark Wrong Lengths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the csv file with wrong lengths\n",
    "wrong_lengths = pd.read_csv('wrong_lengths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create column that will contain wether tjhe length is marked as wrong or not (from manual info in file)\n",
    "all_sessions_with_wrong_lengths = all_sessions\n",
    "all_sessions_with_wrong_lengths['wrong_manual'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# iterate all lengths\n",
    "for index, length in all_sessions_with_wrong_lengths.iterrows():\n",
    "    \n",
    "    length_datetime_string = length['datetime'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # iterate all wrong lengths\n",
    "    for index_wrong, length_wrong in wrong_lengths.iterrows():\n",
    "\n",
    "        # check if the iterated df's row is in the manual list of wrong lengths\n",
    "        if(\n",
    "            length_datetime_string == length_wrong['datetime'] and \\\n",
    "            length['interval'] == str(length_wrong['interval']) and \\\n",
    "            length['length'] == str(length_wrong['length'])\n",
    "          ):\n",
    "            # print('Detected wrong length at index {} : {}-{}-{}'.format(index, length['datetime'], length['interval'], length['length']))\n",
    "            all_sessions_with_wrong_lengths.loc[index,'wrong_manual'] = True\n",
    "\n",
    "all_sessions_with_wrong_lengths[all_sessions_with_wrong_lengths['wrong_manual'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear csv and write header in file where sessions data will be dumped\n",
    "with open('lengths.csv', 'w') as f:\n",
    "    f.write('datetime,interval,length,style,distance,time,strokes,wrong_manual\\n')\n",
    "\n",
    "# dump dataframe into csv\n",
    "all_sessions_with_wrong_lengths.to_csv('lengths.csv', header=False, index=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
